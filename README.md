# ğŸ¤– Nao-OpenAI-Morgan-Assist

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](#-license)
[![Python](https://img.shields.io/badge/Python-2.7_|_3.9+-blue.svg)](#-requirements)
[![Platform](https://img.shields.io/badge/Robot-NAO-orange.svg)]()
[![OpenAI](https://img.shields.io/badge/API-OpenAI-black.svg)]()
[![Pinecone](https://img.shields.io/badge/VectorDB-Pinecone-5B9BD5.svg)]()

A voice-driven assistant that connects the **NAO humanoid robot** to **OpenAI (Whisper + GPT)** and a **Pinecone** knowledge base for the **Morgan State University (MSU) Computer Science Department**.

---

## ğŸ“Œ Overview
**Nao-OpenAI-Morgan-Assist** lets NAO:
- ğŸ¤ **Listen** to users
- ğŸ“ **Transcribe** speech with OpenAI **Whisper**
- ğŸ“‚ **Retrieve** Morgan CS knowledge from **Pinecone**
- ğŸ’¡ **Generate** answers with **GPT**
- ğŸ”Š **Speak** replies via NAO TTS

> Developed by **Aayush Shrestha**.

---

## âœ¨ Features
- ğŸ—£ **Voice Interaction** â€“ Robust on-device capture with silence/VAD handling  
- ğŸ§  **Morgan Chatbot Mode** â€“ Answers from MSU CS knowledge base (Pinecone)  
- ğŸ“š **Study Mode** â€“ Teaches step-by-step with examples + quick practice  
- ğŸ’¬ **General Mode** â€“ Friendly Q&A assistant  
- ğŸ‘¤ **Face Recognition** â€“ Enroll/recognize users (face encodings)  
- ğŸ’¾ **Memory Manager** â€“ Per-user chat history and name recall  
- ğŸ§© **Function Hooks** â€“ Simple server â€œfunction_callâ€ support for actions  

---

## ğŸ—‚ Project Structure
Awesome â€” hereâ€™s a ready-to-paste README.md in proper Markdown (titles, bold, lists, code blocks, badges, the works). Drop this straight into your repo and commit.

# ğŸ¤– Nao-OpenAI-Morgan-Assist

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](#-license)
[![Python](https://img.shields.io/badge/Python-2.7_|_3.9+-blue.svg)](#-requirements)
[![Platform](https://img.shields.io/badge/Robot-NAO-orange.svg)]()
[![OpenAI](https://img.shields.io/badge/API-OpenAI-black.svg)]()
[![Pinecone](https://img.shields.io/badge/VectorDB-Pinecone-5B9BD5.svg)]()

A voice-driven assistant that connects the **NAO humanoid robot** to **OpenAI (Whisper + GPT)** and a **Pinecone** knowledge base for the **Morgan State University (MSU) Computer Science Department**.

---

## ğŸ“Œ Overview
**Nao-OpenAI-Morgan-Assist** lets NAO:
- ğŸ¤ **Listen** to users
- ğŸ“ **Transcribe** speech with OpenAI **Whisper**
- ğŸ“‚ **Retrieve** Morgan CS knowledge from **Pinecone**
- ğŸ’¡ **Generate** answers with **GPT**
- ğŸ”Š **Speak** replies via NAO TTS

> Developed by **Aayush Shrestha**.

---

## âœ¨ Features
- ğŸ—£ **Voice Interaction** â€“ Robust on-device capture with silence/VAD handling  
- ğŸ§  **Morgan Chatbot Mode** â€“ Answers from MSU CS knowledge base (Pinecone)  
- ğŸ“š **Study Mode** â€“ Teaches step-by-step with examples + quick practice  
- ğŸ’¬ **General Mode** â€“ Friendly Q&A assistant  
- ğŸ‘¤ **Face Recognition** â€“ Enroll/recognize users (face encodings)  
- ğŸ’¾ **Memory Manager** â€“ Per-user chat history and name recall  
- ğŸ§© **Function Hooks** â€“ Simple server â€œfunction_callâ€ support for actions  

---

## ğŸ—‚ Project Structure


Nao-OpenAI-Morgan-Assist/
â”œâ”€ main.py # Entry point â€“ wake flow (chat, mininao, chatbot)
â”œâ”€ server.py # Flask backend: Whisper, GPT, Pinecone, Face APIs
â”œâ”€ chatbot_mode.py # Morgan chatbot loop (NAO â‡„ Server)
â”œâ”€ chat_mode.py # Multi-mode chat with gestures + interrupts
â”œâ”€ audio_handler.py # Recording, VAD, trimming, pre-emphasis, AGC
â”œâ”€ memory_manager.py # User profiles & chat history
â”œâ”€ face_store.py # Face encodings (enroll/list/recognize)
â”œâ”€ utils/
â”‚ â”œâ”€ camera_capture.py # Take photos from NAO camera
â”‚ â””â”€ face_utils.py # Face detection/mood helpers
â”œâ”€ requirements.txt # Python deps for the server (Py3)
â”œâ”€ LICENSE # MIT
â””â”€ README.md # You are here


---

## âš™ï¸ Requirements
- **Python 2.7** (NAO side scripts; NAOqi SDK)
- **Python 3.9+** (Flask backend server)
- **NAOqi SDK**
- **OpenAI** Python SDK
- **Flask**
- **Pinecone** client
- **face_recognition** (+ dlib dependencies)

Install server deps:
```bash
pip install -r requirements.txt

ğŸš€ Quick Start
1) Clone
git clone https://github.com/theaayushstha1/Nao-OpenAI-Morgan-Assist.git
cd Nao-OpenAI-Morgan-Assist

2) Configure environment

Create a .env file (in the server directory or repo root where server.py runs):

OPENAI_API_KEY=sk-your-key
PINECONE_API_KEY=pcsk-your-key
PINECONE_INDEX_NAME=vectorized-datasource
PINECONE_NAMESPACE=docs
# Optional:
PINECONE_ENV=us-east-1
WHISPER_MODEL=whisper-1

# NAO defaults (client scripts may also read these):
NAO_IP=192.168.xx.xx
NAO_PORT=9559

3) Run the Flask backend (Python 3)
python server.py
# â‡’ serves on http://0.0.0.0:5000

4) Run on NAO (Python 2.7)
python main.py
# Wake phrases: "let's chat", "mini nao", "morgan assist"

ğŸ”Œ API Endpoints (server)

POST /upload â€“ multipart audio file â†’ Whisper â†’ { user_input, reply, active_mode, â€¦ }

POST /chat_text â€“ JSON {username, text, mode?} â†’ GPT (+ Pinecone when mode == "chatbot")

POST /face/recognize â€“ multipart image â†’ { match, name?, distance }

POST /face/enroll â€“ multipart image + name â†’ { ok, enrolled }

GET /face/list â€“ summary of stored encodings

ğŸ§­ Modes

General â€“ default concise helper

Study â€“ stepwise explanations + mini practice question

Therapist â€“ warm, validating (non-clinical) support

Broker â€“ neutral market concepts (educational only)

Chatbot â€“ Morgan CS knowledge via Pinecone context + GPT

The server auto-detects â€œswitch to â€¦ modeâ€ phrases and keeps your normal sentences intact (e.g., it wonâ€™t strip the word study from â€œI study algorithmsâ€).

ğŸ›  Configuration Tips

Latency tuning (speech end detection): tweak in audio_handler.py

TRAIL_MS (silence tail), POLL_MS, ENERGY_MIN_START/KEEP

Interrupt while speaking (chat mode): user can say â€œstop / skip / nextâ€; the client listens in a side thread and calls tts.stopAll()

Prevent self-hearing: during TTS, temporarily lower input sensitivity or gate by energy threshold; client already filters short clips and uses brief listen windows for interrupts.

â“ FAQ

Why did it not use Pinecone?
Make sure mode is chatbot in the request (client sets this when entering â€œMorgan Assistâ€). The server path if mode == "chatbot": performs embed â†’ Pinecone â†’ GPT with context.

Where do I add MSU CS docs?
Ingest your content into the Pinecone index named by PINECONE_INDEX_NAME/PINECONE_NAMESPACE using the same embedding model (text-embedding-3-small).

It stops recording too fast/too slow.
Adjust TRAIL_MS, NO_SPEECH_TIMEOUT_S, and thresholds in audio_handler.py.

ğŸ–¼ System Diagram

c:\Users\Aayush\Pictures\Screenshots\Screenshot 2025-09-24 144932.png


ğŸ§ª Minimal Test
# Server health
curl http://localhost:5000/test
# => {"message":"Test route working!"}

ğŸ“œ License

Released under the MIT License. See LICENSE
.

ğŸ‘¨â€ğŸ’» Author

Aayush Shrestha â€” Lead Developer